# Topic: Lexer & Scanner

### Course: Formal Languages & Finite Automata

### Author: Pascal Adrian

----

## Objectives:

1. Understand what lexical analysis is.
2. Get familiar with the inner workings of a lexer/scanner/tokenizer.
3. Implement a sample lexer and show how it works.

----

## Theoretical Notes:

### Lexer (Lexical Analyzer):
The lexer takes the source code as input and breaks it down into smaller units called tokens.
Tokens are the smallest meaningful units of the source code, such as keywords, identifiers, operators, literals (like numbers and strings), and punctuation symbols.
The lexer identifies these tokens by analyzing the characters of the source code according to a set of rules defined by the programming language's syntax.
For example, in the code snippet int x = 5 + 3;, the lexer might generate tokens like INT, IDENTIFIER(x), ASSIGN, NUMBER(5), PLUS, NUMBER(3), and SEMICOLON.
### Parser:
The parser takes the tokens generated by the lexer and organizes them into a hierarchical structure called a parse tree or abstract syntax tree (AST).
The parse tree represents the syntactic structure of the source code, showing how the different tokens relate to each other in terms of the language's grammar rules.
The parser ensures that the arrangement of tokens conforms to the rules of the programming language's syntax.
It checks for syntactic correctness and may perform additional tasks like type checking or building intermediate representations of the code for optimization purposes.
For example, the parser might organize the tokens generated by the lexer into a parse tree that represents the assignment statement x = 5 + 3;, with nodes for assignment, addition operation, and integer literals.

----

## Practical Implementation:

1. First I declared a map of token types and their corresponding regular expressions. Also I declared an array keywords 
that contains all the valid keywords.

```
TOKENS = {
    'IDENTIFIER': r'[a-zA-Z][a-zA-Z0-9_]*',
    'INTEGER': r'\d+',
    'FLOAT': r'\d+\.\d+',
    'DOT': r'\.',
    'COMMA': r',',
    'STRING': r'\'[a-zA-Z\s][a-zA-Z\s]+\'',
    'CHAR': r'\'[a-zA-Z]\'',
    'INCREMENT': r'\+=',
    'PLUS': r'\+',
    'MINUS': r'-',
    'MULTIPLY': r'\*',
    'DIVIDE': r'/',
    'LPAREN': r'\(',
    'RPAREN': r'\)',
    'EQUAL': r'==',
    'LESS_EQUAL': r'<=',
    'GREATER_EQUAL': r'>=',
    'LESS': r'<',
    'GREATER': r'>',
    'ASSIGN': r'=',
    'COLON': r':',
    'SPACE': r'\s+',
    'NEWLINE': r'\n'
}

KEY_WORDS = ['if', 'else', 'while', 'for', 'return', 'break', 'continue']
```

2. Then I created a class for the token, which has a type and a value. The type is the type of the token, 
and the value is the actual value of the token.

```
class Token:
    def __init__(self, type, value):
        self.type = type
        self.value = value

    def __str__(self):
        return f"Token({self.type}, {self.value})"

    def __repr__(self):
        return self.__str__()
```

3. Then I created a class for the ```Lexer```, which is responsible for generating tokens from the input source code. 
It uses the TOKENS map and the KEY_WORDS array to identify the tokens in the source code. Also, if it encounters a 
invalid token, it raises an error.

```
class Lexer:
    def __init__(self, text):
        self.text = text
        self.pos = 0
        self.current_char = self.text[self.pos]

    def error(self):
        raise Exception("Invalid character")

    def get_next_token(self):
        while self.pos < len(self.text):
            for token_type, pattern in TOKENS.items():
                regex = re.compile(pattern)
                match = regex.match(self.text, self.pos)
                if match:
                    value = match.group(0)
                    self.pos = match.end()
                    if token_type == 'SPACE' or token_type == 'NEWLINE':
                        break  # Skip spaces
                    elif value in KEY_WORDS:
                        return Token(value.upper(), value)
                    else:
                        return Token(token_type, value)
            else:
                self.error()

        return Token('EOF', None)
```

4. After this, I created a ```Scanner``` class, which is gets the string at creation and then uses the ```Lexer``` to
generate tokens from the source code. It also has a method to print the tokens.

```
class Parser:
    def __init__(self, text):
        self.text = text
        self.tokens = []
        pass

    def parse(self):
        lexer = Lexer(self.text)
        while True:
            token = lexer.get_next_token()
            if token.type == 'EOF':
                break
            self.tokens.append(token)

    def print_tokens(self):
        for token in self.tokens:
            print(token)
```

5. Finally, I created a infinite loop to get the input from the user and then parse and print the tokens.

```
while True:
    try:
        text = input("Enter the source code: ")
        parser = Parser(text)
        parser.parse()
        parser.print_tokens()
    except Exception as e:
        print(e)
```

## Conclusion:

In conclusion, delving into the realm of lexers and scanners has provided invaluable insights into the fundamental stages of language processing. Through understanding lexical analysis, we've grasped the intricate process of breaking down source code into meaningful tokens, laying the groundwork for subsequent parsing and interpretation. The theoretical foundation laid out elucidates the roles of lexers and parsers in comprehensively analyzing syntax and structure.

Moreover, the practical implementation showcased the translation of theoretical knowledge into functional code, exemplifying the utilization of regular expressions and tokenization techniques. By creating a lexer and scanner, we've not only cemented our understanding but also gained hands-on experience in developing tools pivotal to compiler construction.

This exploration underscores the significance of formal languages and finite automata in software development, equipping us with essential skills for crafting efficient and robust language processors.




